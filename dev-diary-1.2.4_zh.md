# v1.2.4 开发日记：构建采坑与架构取舍

## 前言

发布 v1.2.4 版本的过程充满了挑战与取舍。作为一个跨平台应用（macOS & Windows），在追求原生体验与高性能的同时，不可避免地会遇到平台特有的限制。本篇日记记录了在构建过程中遇到的两个主要“坑”以及相应的架构决策，希望能对同样在折腾 Electron 开发的朋友们有所启发。

## 1. 🍎 macOS 构建：Security-Scoped Bookmarks 的那些事

在为 Mac App Store (MAS) 构建版本时，最大的挑战来源于 Apple 严格的沙盒（Sandbox）机制。

### 问题描述
为了符合 MAS 的上架要求，应用必须开启沙盒环境。然而，沙盒环境限制了应用对文件系统的任意访问。即使用户通过 `dialog.showOpenDialog` 明确选择了某个文件夹，应用也只能在当次运行中拥有访问权限。一旦应用重启，权限就会丢失，导致用户无法再次打开之前添加的文件夹。

### 解决方案
为了解决这个问题，必须实现 **Security-Scoped Bookmarks**。

1.  **获取权限时保存 Bookmark**：当用户选择文件夹时，通过 Electron 的 API 获取该路径的 `bookmark`（由 macOS 系统生成的一串加密数据）。
2.  **持久化存储**：将这个 `bookmark` 字符串存储在本地数据库（如 LowDB）中。
3.  **重启后恢复权限**：应用启动时，读取存储的 `bookmark`，并调用 `app.startAccessingSecurityScopedResource` 来重新激活访问权限。

这个机制确保了用户在重启应用后，依然可以流畅地访问他们的笔记库，既满足了安全性，又保证了用户体验。在 v1.2.4 中，我们终于彻底修复了 MAS 版本下的文件夹权限问题。

## 2. 🪟 Windows 构建：忍痛割爱 WebLLM

相对于 macOS 的权限斗争，Windows 版本的挑战则更多在于性能与架构的选择。

### 架构决策
在 v1.2.4 的 Windows 版本中，我们做出了一个艰难的决定：**移除内置的 WebLLM 引擎支持**。

### 原因分析
1.  **性能瓶颈**：WebLLM 依赖于 WebGPU 技术在浏览器环境中运行大模型。虽然在 macOS（尤其是 Apple Silicon 芯片）上表现尚可，但在 Windows 平台上，由于硬件配置千差万别，驱动兼容性复杂，导致 WebLLM 的加载速度慢、推理延迟高，甚至在很多中低端机器上直接崩溃。
2.  **用户体验**：为了维持一个不稳定的功能而牺牲整体应用的流畅度和稳定性是不划算的。用户期待的是一个“开箱即用”且响应迅速的工具，而不是一个需要漫长等待加载的试验品。
3.  **替代方案**：我们强烈推荐 Windows 用户使用 **Ollama**。作为一个独立的本地推理引擎，Ollama 在 Windows 上运行非常稳定且高效。不仅支持的模型更多（如 Llama 3, Qwen 2.5 等），而且资源占用控制得更好。

### 结论
虽然少了一个“内置引擎”的卖点，但通过引导用户使用架构更成熟的 Ollama，实际上提升了最终的生产力体验。架构选择往往就是这样，没有绝对的完美，只有在特定场景下最合适的权衡。

---

*以上不仅仅是技术记录，也是智简 WitNote 走向更成熟、更稳定的一步。感谢每一位用户的包容与反馈。*
