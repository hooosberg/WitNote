/**
 * useLLM Hook
 * æ ¸å¿ƒï¼šåŒå¼•æ“ç®¡ç†ï¼Œè‡ªåŠ¨æ£€æµ‹ Ollama å¹¶æ™ºèƒ½åˆ‡æ¢
 */

import { useState, useEffect, useCallback, useRef } from 'react';
import {
    LLMProviderType,
    LLMMessage,
    ChatMessage,
    LLMStatus,
    LoadProgress,
    OllamaModel,
    DEFAULT_WEBLLM_MODEL
} from '../services/types';
import { OllamaService } from '../services/OllamaService';
import { WebLLMService } from '../services/WebLLMService';

export interface UseLLMReturn {
    // çŠ¶æ€
    providerType: LLMProviderType;
    status: LLMStatus;
    modelName: string;
    loadProgress: LoadProgress | null;

    // Ollama ç›¸å…³
    ollamaModels: OllamaModel[];
    selectedOllamaModel: string;
    setSelectedOllamaModel: (model: string) => void;

    // èŠå¤©ç›¸å…³
    messages: ChatMessage[];
    isGenerating: boolean;

    // æ–¹æ³•
    sendMessage: (content: string) => Promise<void>;
    abortGeneration: () => void;
    clearMessages: () => void;
    retryDetection: () => void;
}

// ç”Ÿæˆå”¯ä¸€ ID
function generateId(): string {
    return `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
}

export function useLLM(): UseLLMReturn {
    // æä¾›è€…çŠ¶æ€
    const [providerType, setProviderType] = useState<LLMProviderType>('webllm');
    const [status, setStatus] = useState<LLMStatus>('detecting');
    const [modelName, setModelName] = useState<string>('');
    const [loadProgress, setLoadProgress] = useState<LoadProgress | null>(null);

    // Ollama çŠ¶æ€
    const [ollamaModels, setOllamaModels] = useState<OllamaModel[]>([]);
    const [selectedOllamaModel, setSelectedOllamaModel] = useState<string>('');

    // èŠå¤©çŠ¶æ€
    const [messages, setMessages] = useState<ChatMessage[]>([]);
    const [isGenerating, setIsGenerating] = useState(false);

    // æœåŠ¡å¼•ç”¨
    const ollamaServiceRef = useRef<OllamaService | null>(null);
    const webllmServiceRef = useRef<WebLLMService | null>(null);

    /**
     * æ£€æµ‹å¹¶åˆå§‹åŒ– LLM å¼•æ“
     */
    const detectAndInitialize = useCallback(async () => {
        console.log('ğŸ” å¼€å§‹æ£€æµ‹ LLM å¼•æ“...');
        setStatus('detecting');
        setLoadProgress(null);

        // Step 1: å°è¯•æ¢æµ‹ Ollama
        const models = await OllamaService.detect();

        if (models && models.length > 0) {
            // Case A: Ollama åœ¨çº¿ä¸”æœ‰æ¨¡å‹
            console.log('âœ… Ollama Detected: YES');
            console.log(`ğŸ“‹ å¯ç”¨æ¨¡å‹: ${models.map(m => m.name).join(', ')}`);

            setOllamaModels(models);
            setSelectedOllamaModel(models[0].name);
            setProviderType('ollama');
            setModelName(models[0].name);

            // åˆå§‹åŒ– Ollama æœåŠ¡
            const ollamaService = new OllamaService(models[0].name);
            try {
                await ollamaService.initialize();
                ollamaServiceRef.current = ollamaService;
                setStatus('ready');

                // è‡ªæµ‹æ¶ˆæ¯
                console.log('ğŸ§ª Ollama è‡ªæµ‹é€šè¿‡');
            } catch (error) {
                console.error('âŒ Ollama åˆå§‹åŒ–å¤±è´¥ï¼Œé™çº§åˆ° WebLLM:', error);
                await initializeWebLLM();
            }
        } else {
            // Case B: Ollama ç¦»çº¿æˆ–æ— æ¨¡å‹
            console.log('âš ï¸ Ollama Detected: NO, falling back to WebLLM');
            await initializeWebLLM();
        }
    }, []);

    /**
     * åˆå§‹åŒ– WebLLM
     */
    const initializeWebLLM = async () => {
        setProviderType('webllm');
        setModelName(DEFAULT_WEBLLM_MODEL);
        setStatus('loading');

        const webllmService = new WebLLMService();

        // è®¾ç½®è¿›åº¦å›è°ƒ
        webllmService.setProgressCallback((progress) => {
            setLoadProgress(progress);
            console.log(`ğŸ“¥ åŠ è½½è¿›åº¦: ${progress.progress}% - ${progress.text}`);
        });

        try {
            await webllmService.initialize();
            webllmServiceRef.current = webllmService;
            setStatus('ready');
            setLoadProgress(null);

            console.log('ğŸ§ª WebLLM è‡ªæµ‹é€šè¿‡');
        } catch (error) {
            console.error('âŒ WebLLM åˆå§‹åŒ–å¤±è´¥:', error);
            setStatus('error');
        }
    };

    /**
     * å‘é€æ¶ˆæ¯
     */
    const sendMessage = useCallback(async (content: string) => {
        if (!content.trim() || isGenerating) return;
        if (status !== 'ready') {
            console.warn('âš ï¸ LLM æœåŠ¡æœªå°±ç»ª');
            return;
        }

        // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        const userMessage: ChatMessage = {
            id: generateId(),
            role: 'user',
            content: content.trim(),
            timestamp: Date.now()
        };

        // æ·»åŠ ç©ºçš„åŠ©æ‰‹æ¶ˆæ¯ï¼ˆç”¨äºæµå¼å¡«å……ï¼‰
        const assistantMessage: ChatMessage = {
            id: generateId(),
            role: 'assistant',
            content: '',
            timestamp: Date.now(),
            isStreaming: true
        };

        setMessages(prev => [...prev, userMessage, assistantMessage]);
        setIsGenerating(true);

        // å‡†å¤‡å†å²æ¶ˆæ¯
        const historyMessages: LLMMessage[] = messages.map(m => ({
            role: m.role,
            content: m.content
        }));
        historyMessages.push({ role: 'user', content: content.trim() });

        // æµå¼å›è°ƒ
        const onToken = (token: string) => {
            setMessages(prev => {
                const updated = [...prev];
                const lastMsg = updated[updated.length - 1];
                if (lastMsg && lastMsg.role === 'assistant') {
                    lastMsg.content += token;
                }
                return updated;
            });
        };

        const onComplete = () => {
            setMessages(prev => {
                const updated = [...prev];
                const lastMsg = updated[updated.length - 1];
                if (lastMsg) {
                    lastMsg.isStreaming = false;
                }
                return updated;
            });
            setIsGenerating(false);
        };

        const onError = (error: Error) => {
            console.error('âŒ ç”Ÿæˆé”™è¯¯:', error);
            setMessages(prev => {
                const updated = [...prev];
                const lastMsg = updated[updated.length - 1];
                if (lastMsg && lastMsg.role === 'assistant') {
                    lastMsg.content = `âŒ ç”Ÿæˆå‡ºé”™: ${error.message}`;
                    lastMsg.isStreaming = false;
                }
                return updated;
            });
            setIsGenerating(false);
        };

        // æ ¹æ®å½“å‰æä¾›è€…è°ƒç”¨å¯¹åº”æœåŠ¡
        try {
            if (providerType === 'ollama' && ollamaServiceRef.current) {
                await ollamaServiceRef.current.streamChat(historyMessages, onToken, onComplete, onError);
            } else if (providerType === 'webllm' && webllmServiceRef.current) {
                await webllmServiceRef.current.streamChat(historyMessages, onToken, onComplete, onError);
            } else {
                throw new Error('æ²¡æœ‰å¯ç”¨çš„ LLM æœåŠ¡');
            }
        } catch (error) {
            onError(error instanceof Error ? error : new Error('æœªçŸ¥é”™è¯¯'));
        }
    }, [messages, isGenerating, status, providerType]);

    /**
     * ä¸­æ­¢ç”Ÿæˆ
     */
    const abortGeneration = useCallback(() => {
        if (providerType === 'ollama' && ollamaServiceRef.current) {
            ollamaServiceRef.current.abort();
        } else if (providerType === 'webllm' && webllmServiceRef.current) {
            webllmServiceRef.current.abort();
        }
        setIsGenerating(false);
    }, [providerType]);

    /**
     * æ¸…ç©ºæ¶ˆæ¯
     */
    const clearMessages = useCallback(() => {
        setMessages([]);
    }, []);

    /**
     * é‡æ–°æ£€æµ‹
     */
    const retryDetection = useCallback(() => {
        detectAndInitialize();
    }, [detectAndInitialize]);

    /**
     * åˆ‡æ¢ Ollama æ¨¡å‹
     */
    const handleSetSelectedOllamaModel = useCallback((model: string) => {
        setSelectedOllamaModel(model);
        setModelName(model);

        if (ollamaServiceRef.current) {
            ollamaServiceRef.current.setModel(model);
        }
    }, []);

    // å¯åŠ¨æ—¶æ£€æµ‹
    useEffect(() => {
        detectAndInitialize();

        // æ¸…ç†
        return () => {
            if (webllmServiceRef.current) {
                webllmServiceRef.current.destroy();
            }
        };
    }, [detectAndInitialize]);

    return {
        providerType,
        status,
        modelName,
        loadProgress,
        ollamaModels,
        selectedOllamaModel,
        setSelectedOllamaModel: handleSetSelectedOllamaModel,
        messages,
        isGenerating,
        sendMessage,
        abortGeneration,
        clearMessages,
        retryDetection
    };
}

export default useLLM;
