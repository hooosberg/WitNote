/**
 * WebLLM Web Worker
 * åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ WebLLM å¼•æ“ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
 */

import * as webllm from '@mlc-ai/web-llm';
import { WorkerMessage, SYSTEM_PROMPT, DEFAULT_WEBLLM_MODEL, LLMMessage } from './types';

let engine: webllm.MLCEngine | null = null;

// å‘é€æ¶ˆæ¯åˆ°ä¸»çº¿ç¨‹
function postMessage(message: WorkerMessage) {
    self.postMessage(message);
}

// åˆå§‹åŒ–å¼•æ“
async function initEngine(modelId: string) {
    try {
        console.log(`ğŸ”„ Worker: å¼€å§‹åŠ è½½æ¨¡å‹ ${modelId}`);

        engine = new webllm.MLCEngine();

        await engine.reload(modelId, {
            // è¿›åº¦å›è°ƒ
            initProgressCallback: (progress) => {
                postMessage({
                    type: 'progress',
                    payload: {
                        stage: progress.text,
                        progress: Math.round(progress.progress * 100),
                        text: progress.text
                    }
                });
            }
        });

        console.log('âœ… Worker: æ¨¡å‹åŠ è½½å®Œæˆ');
        postMessage({ type: 'ready' });
    } catch (error) {
        console.error('âŒ Worker: æ¨¡å‹åŠ è½½å¤±è´¥:', error);
        postMessage({
            type: 'error',
            payload: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
        });
    }
}

// æµå¼èŠå¤©
async function streamChat(messages: LLMMessage[]) {
    if (!engine) {
        postMessage({
            type: 'error',
            payload: 'å¼•æ“æœªåˆå§‹åŒ–'
        });
        return;
    }

    try {
        // æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        const fullMessages: webllm.ChatCompletionMessageParam[] = [
            { role: 'system', content: SYSTEM_PROMPT },
            ...messages.map(m => ({
                role: m.role as 'user' | 'assistant' | 'system',
                content: m.content
            }))
        ];

        const asyncChunkGenerator = await engine.chat.completions.create({
            messages: fullMessages,
            stream: true,
            temperature: 0.7,
            max_tokens: 1024
        });

        for await (const chunk of asyncChunkGenerator) {
            const delta = chunk.choices[0]?.delta?.content;
            if (delta) {
                postMessage({
                    type: 'token',
                    payload: delta
                });
            }
        }

        postMessage({ type: 'complete' });
    } catch (error) {
        console.error('âŒ Worker: ç”Ÿæˆå¤±è´¥:', error);
        postMessage({
            type: 'error',
            payload: error instanceof Error ? error.message : 'ç”Ÿæˆå¤±è´¥'
        });
    }
}

// ä¸­æ­¢ç”Ÿæˆ
function abortGeneration() {
    if (engine) {
        engine.interruptGenerate();
        console.log('ğŸ›‘ Worker: ç”Ÿæˆå·²ä¸­æ­¢');
    }
}

// ç›‘å¬ä¸»çº¿ç¨‹æ¶ˆæ¯
self.onmessage = async (event: MessageEvent<WorkerMessage>) => {
    const { type, payload } = event.data;

    switch (type) {
        case 'init':
            const modelId = (payload as { modelId: string })?.modelId || DEFAULT_WEBLLM_MODEL;
            await initEngine(modelId);
            break;

        case 'chat':
            const messages = payload as LLMMessage[];
            await streamChat(messages);
            break;

        case 'abort':
            abortGeneration();
            break;

        default:
            console.warn('Worker: æœªçŸ¥æ¶ˆæ¯ç±»å‹:', type);
    }
};

console.log('ğŸ§µ WebLLM Worker å·²å¯åŠ¨');
